{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "826a0897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5741aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bilal\\AppData\\Local\\Temp\\ipykernel_8452\\2671051389.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(\"ECG200_TRAIN.csv\", delimiter=\"  \")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-1.0000000e+00</th>\n",
       "      <th>5.0205548e-01</th>\n",
       "      <th>5.4216265e-01</th>\n",
       "      <th>7.2238348e-01</th>\n",
       "      <th>1.4288852e+00</th>\n",
       "      <th>2.1365158e+00</th>\n",
       "      <th>2.2811490e+00</th>\n",
       "      <th>1.9362737e+00</th>\n",
       "      <th>1.4688900e+00</th>\n",
       "      <th>1.0088451e+00</th>\n",
       "      <th>...</th>\n",
       "      <th>9.3104294e-01</th>\n",
       "      <th>6.1029836e-01</th>\n",
       "      <th>6.3889427e-01</th>\n",
       "      <th>6.8467857e-01</th>\n",
       "      <th>5.8323764e-01</th>\n",
       "      <th>6.4052167e-01</th>\n",
       "      <th>7.0858515e-01</th>\n",
       "      <th>7.0501088e-01</th>\n",
       "      <th>7.1381545e-01</th>\n",
       "      <th>4.3376464e-01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.147647</td>\n",
       "      <td>0.804668</td>\n",
       "      <td>0.367771</td>\n",
       "      <td>0.243894</td>\n",
       "      <td>0.026614</td>\n",
       "      <td>-0.274402</td>\n",
       "      <td>0.096731</td>\n",
       "      <td>-0.747731</td>\n",
       "      <td>-1.609777</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.533503</td>\n",
       "      <td>-0.400228</td>\n",
       "      <td>0.176084</td>\n",
       "      <td>1.111768</td>\n",
       "      <td>2.438428</td>\n",
       "      <td>2.734889</td>\n",
       "      <td>1.736054</td>\n",
       "      <td>0.036857</td>\n",
       "      <td>-1.265074</td>\n",
       "      <td>-0.208024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.316646</td>\n",
       "      <td>0.243199</td>\n",
       "      <td>0.370471</td>\n",
       "      <td>1.063738</td>\n",
       "      <td>1.678187</td>\n",
       "      <td>1.759558</td>\n",
       "      <td>1.697717</td>\n",
       "      <td>1.612159</td>\n",
       "      <td>1.168188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.764229</td>\n",
       "      <td>0.610621</td>\n",
       "      <td>0.552900</td>\n",
       "      <td>0.566786</td>\n",
       "      <td>0.604002</td>\n",
       "      <td>0.777068</td>\n",
       "      <td>0.812345</td>\n",
       "      <td>0.748848</td>\n",
       "      <td>0.818042</td>\n",
       "      <td>0.539347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.168874</td>\n",
       "      <td>2.075901</td>\n",
       "      <td>1.760141</td>\n",
       "      <td>1.606446</td>\n",
       "      <td>1.949046</td>\n",
       "      <td>1.302842</td>\n",
       "      <td>0.459332</td>\n",
       "      <td>0.516412</td>\n",
       "      <td>0.852180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419006</td>\n",
       "      <td>0.723888</td>\n",
       "      <td>1.323947</td>\n",
       "      <td>2.136488</td>\n",
       "      <td>1.746597</td>\n",
       "      <td>1.470220</td>\n",
       "      <td>1.893512</td>\n",
       "      <td>1.256949</td>\n",
       "      <td>0.800407</td>\n",
       "      <td>0.731540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.648658</td>\n",
       "      <td>0.752026</td>\n",
       "      <td>2.636231</td>\n",
       "      <td>3.455716</td>\n",
       "      <td>2.118157</td>\n",
       "      <td>0.520620</td>\n",
       "      <td>-0.188627</td>\n",
       "      <td>0.780818</td>\n",
       "      <td>0.933775</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097869</td>\n",
       "      <td>-0.136787</td>\n",
       "      <td>-0.340237</td>\n",
       "      <td>-0.089441</td>\n",
       "      <td>-0.080297</td>\n",
       "      <td>-0.192584</td>\n",
       "      <td>-0.304704</td>\n",
       "      <td>-0.454556</td>\n",
       "      <td>0.314590</td>\n",
       "      <td>0.582190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.404733</td>\n",
       "      <td>1.280859</td>\n",
       "      <td>2.515148</td>\n",
       "      <td>1.299519</td>\n",
       "      <td>1.453432</td>\n",
       "      <td>0.474275</td>\n",
       "      <td>-1.396562</td>\n",
       "      <td>-0.647081</td>\n",
       "      <td>0.431945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.376469</td>\n",
       "      <td>0.277811</td>\n",
       "      <td>0.225676</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.408354</td>\n",
       "      <td>0.540015</td>\n",
       "      <td>-0.027791</td>\n",
       "      <td>0.203476</td>\n",
       "      <td>0.346964</td>\n",
       "      <td>0.339185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.581277</td>\n",
       "      <td>0.876188</td>\n",
       "      <td>1.042767</td>\n",
       "      <td>1.796120</td>\n",
       "      <td>2.541399</td>\n",
       "      <td>2.246653</td>\n",
       "      <td>1.500387</td>\n",
       "      <td>1.031521</td>\n",
       "      <td>0.382672</td>\n",
       "      <td>...</td>\n",
       "      <td>1.002770</td>\n",
       "      <td>0.907869</td>\n",
       "      <td>0.916457</td>\n",
       "      <td>0.923975</td>\n",
       "      <td>0.767357</td>\n",
       "      <td>0.656223</td>\n",
       "      <td>0.762357</td>\n",
       "      <td>0.501373</td>\n",
       "      <td>-0.333336</td>\n",
       "      <td>-0.524546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.689017</td>\n",
       "      <td>2.708703</td>\n",
       "      <td>2.008381</td>\n",
       "      <td>2.235800</td>\n",
       "      <td>1.516982</td>\n",
       "      <td>0.029916</td>\n",
       "      <td>-0.561346</td>\n",
       "      <td>-0.793702</td>\n",
       "      <td>-0.979371</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.136610</td>\n",
       "      <td>-0.072176</td>\n",
       "      <td>-0.082738</td>\n",
       "      <td>-0.138468</td>\n",
       "      <td>-0.120396</td>\n",
       "      <td>-0.089411</td>\n",
       "      <td>-0.243141</td>\n",
       "      <td>-0.119710</td>\n",
       "      <td>0.124042</td>\n",
       "      <td>0.273463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.197677</td>\n",
       "      <td>0.455417</td>\n",
       "      <td>0.973110</td>\n",
       "      <td>1.935956</td>\n",
       "      <td>2.259463</td>\n",
       "      <td>1.741341</td>\n",
       "      <td>1.158296</td>\n",
       "      <td>0.418241</td>\n",
       "      <td>-0.071605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.482452</td>\n",
       "      <td>0.325569</td>\n",
       "      <td>0.247991</td>\n",
       "      <td>0.184127</td>\n",
       "      <td>0.050358</td>\n",
       "      <td>0.241988</td>\n",
       "      <td>0.331451</td>\n",
       "      <td>-0.120006</td>\n",
       "      <td>0.042423</td>\n",
       "      <td>0.343293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.179500</td>\n",
       "      <td>1.038409</td>\n",
       "      <td>1.946421</td>\n",
       "      <td>2.705141</td>\n",
       "      <td>1.670706</td>\n",
       "      <td>-0.101167</td>\n",
       "      <td>-1.578876</td>\n",
       "      <td>-0.750906</td>\n",
       "      <td>0.175310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324323</td>\n",
       "      <td>0.330489</td>\n",
       "      <td>0.111953</td>\n",
       "      <td>0.448948</td>\n",
       "      <td>0.567132</td>\n",
       "      <td>0.136757</td>\n",
       "      <td>0.444768</td>\n",
       "      <td>0.151050</td>\n",
       "      <td>0.193378</td>\n",
       "      <td>0.451709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.073124</td>\n",
       "      <td>0.776054</td>\n",
       "      <td>2.181336</td>\n",
       "      <td>3.440325</td>\n",
       "      <td>2.168475</td>\n",
       "      <td>0.497315</td>\n",
       "      <td>-0.924284</td>\n",
       "      <td>-1.499227</td>\n",
       "      <td>-0.679328</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058935</td>\n",
       "      <td>-0.130638</td>\n",
       "      <td>-0.347235</td>\n",
       "      <td>-0.177933</td>\n",
       "      <td>-0.060332</td>\n",
       "      <td>-0.347634</td>\n",
       "      <td>-0.447443</td>\n",
       "      <td>-0.066689</td>\n",
       "      <td>-0.178448</td>\n",
       "      <td>-0.256052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows Ã— 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    -1.0000000e+00   5.0205548e-01   5.4216265e-01   7.2238348e-01  \\\n",
       "0              1.0        0.147647        0.804668        0.367771   \n",
       "1             -1.0        0.316646        0.243199        0.370471   \n",
       "2             -1.0        1.168874        2.075901        1.760141   \n",
       "3              1.0        0.648658        0.752026        2.636231   \n",
       "4              1.0        0.404733        1.280859        2.515148   \n",
       "..             ...             ...             ...             ...   \n",
       "94             1.0        0.581277        0.876188        1.042767   \n",
       "95            -1.0        2.689017        2.708703        2.008381   \n",
       "96            -1.0        0.197677        0.455417        0.973110   \n",
       "97             1.0        0.179500        1.038409        1.946421   \n",
       "98             1.0        0.073124        0.776054        2.181336   \n",
       "\n",
       "     1.4288852e+00   2.1365158e+00   2.2811490e+00   1.9362737e+00  \\\n",
       "0         0.243894        0.026614       -0.274402        0.096731   \n",
       "1         1.063738        1.678187        1.759558        1.697717   \n",
       "2         1.606446        1.949046        1.302842        0.459332   \n",
       "3         3.455716        2.118157        0.520620       -0.188627   \n",
       "4         1.299519        1.453432        0.474275       -1.396562   \n",
       "..             ...             ...             ...             ...   \n",
       "94        1.796120        2.541399        2.246653        1.500387   \n",
       "95        2.235800        1.516982        0.029916       -0.561346   \n",
       "96        1.935956        2.259463        1.741341        1.158296   \n",
       "97        2.705141        1.670706       -0.101167       -1.578876   \n",
       "98        3.440325        2.168475        0.497315       -0.924284   \n",
       "\n",
       "     1.4688900e+00   1.0088451e+00  ...   9.3104294e-01   6.1029836e-01  \\\n",
       "0        -0.747731       -1.609777  ...       -0.533503       -0.400228   \n",
       "1         1.612159        1.168188  ...        0.764229        0.610621   \n",
       "2         0.516412        0.852180  ...        0.419006        0.723888   \n",
       "3         0.780818        0.933775  ...       -0.097869       -0.136787   \n",
       "4        -0.647081        0.431945  ...        0.376469        0.277811   \n",
       "..             ...             ...  ...             ...             ...   \n",
       "94        1.031521        0.382672  ...        1.002770        0.907869   \n",
       "95       -0.793702       -0.979371  ...       -0.136610       -0.072176   \n",
       "96        0.418241       -0.071605  ...        0.482452        0.325569   \n",
       "97       -0.750906        0.175310  ...        0.324323        0.330489   \n",
       "98       -1.499227       -0.679328  ...       -0.058935       -0.130638   \n",
       "\n",
       "     6.3889427e-01   6.8467857e-01   5.8323764e-01   6.4052167e-01  \\\n",
       "0         0.176084        1.111768        2.438428        2.734889   \n",
       "1         0.552900        0.566786        0.604002        0.777068   \n",
       "2         1.323947        2.136488        1.746597        1.470220   \n",
       "3        -0.340237       -0.089441       -0.080297       -0.192584   \n",
       "4         0.225676        0.159091        0.408354        0.540015   \n",
       "..             ...             ...             ...             ...   \n",
       "94        0.916457        0.923975        0.767357        0.656223   \n",
       "95       -0.082738       -0.138468       -0.120396       -0.089411   \n",
       "96        0.247991        0.184127        0.050358        0.241988   \n",
       "97        0.111953        0.448948        0.567132        0.136757   \n",
       "98       -0.347235       -0.177933       -0.060332       -0.347634   \n",
       "\n",
       "     7.0858515e-01   7.0501088e-01   7.1381545e-01   4.3376464e-01  \n",
       "0         1.736054        0.036857       -1.265074       -0.208024  \n",
       "1         0.812345        0.748848        0.818042        0.539347  \n",
       "2         1.893512        1.256949        0.800407        0.731540  \n",
       "3        -0.304704       -0.454556        0.314590        0.582190  \n",
       "4        -0.027791        0.203476        0.346964        0.339185  \n",
       "..             ...             ...             ...             ...  \n",
       "94        0.762357        0.501373       -0.333336       -0.524546  \n",
       "95       -0.243141       -0.119710        0.124042        0.273463  \n",
       "96        0.331451       -0.120006        0.042423        0.343293  \n",
       "97        0.444768        0.151050        0.193378        0.451709  \n",
       "98       -0.447443       -0.066689       -0.178448       -0.256052  \n",
       "\n",
       "[99 rows x 97 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"ECG200_TRAIN.csv\", delimiter=\"  \")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4581a021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0000000e+00    0\n",
       " 5.0205548e-01    0\n",
       " 5.4216265e-01    0\n",
       " 7.2238348e-01    0\n",
       " 1.4288852e+00    0\n",
       "                 ..\n",
       " 6.4052167e-01    0\n",
       " 7.0858515e-01    0\n",
       " 7.0501088e-01    0\n",
       " 7.1381545e-01    0\n",
       " 4.3376464e-01    0\n",
       "Length: 97, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d4b94e",
   "metadata": {},
   "source": [
    "## Sparating the dataset with labels -1s and 1s, 70% for ones and 30% for the minus ones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b93084b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = df.sort_values(by='-1.0000000e+00',ignore_index=True)\n",
    "af = np.where(s['-1.0000000e+00'] == -1)\n",
    "minusones = s[:30]\n",
    "ones = s[30:]\n",
    "kfold = []\n",
    "len(ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a843bef0",
   "metadata": {},
   "source": [
    "## Making 4 patches of 1s and -1s\n",
    "patches1 --> 1s  \n",
    "\n",
    "patches2 --> -1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5cf2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 0\n",
    "patches1 = []\n",
    "for i in range(17,len(ones)+1,17):\n",
    "    k = ones[g:i]\n",
    "    patches1.append(k)\n",
    "    g = i\n",
    "    \n",
    "g = 0\n",
    "patches2 = []\n",
    "for i in range(7,len(minusones)+1,7):\n",
    "    k = minusones[g:i]\n",
    "    patches2.append(k)\n",
    "    g = i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d53af9e",
   "metadata": {},
   "source": [
    "Concatenating the datesets (patches) of 1s and -1s and shuffling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7ed5b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,4):\n",
    "    biji = pd.concat([patches1[i],patches2[i]],axis=0)\n",
    "    kfold.append(biji)\n",
    "\n",
    "# shuffleing\n",
    "for i in range(0,4):\n",
    "    kfold[i] = kfold[i].sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc1da8e",
   "metadata": {},
   "source": [
    "## Calculating the accuracies of differnt K-folds on machine learning Model (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "430e9b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7bc7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = kfold[0]\n",
    "train = pd.concat([kfold[1],kfold[2],kfold[3]],axis=0)\n",
    "X_train = train.drop('-1.0000000e+00',axis=1)\n",
    "y_train = train[\"-1.0000000e+00\"]\n",
    "\n",
    "X_test = test.drop('-1.0000000e+00',axis=1)\n",
    "y_test = test[\"-1.0000000e+00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f7943a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY:  0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators = 100)  \n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "from sklearn import metrics \n",
    "print(\"ACCURACY: \", metrics.accuracy_score(y_test, y_pred))\n",
    "accuracies.append(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c396dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = kfold[1]\n",
    "train = pd.concat([kfold[0],kfold[2],kfold[3]],axis=0)\n",
    "X_train = train.drop('-1.0000000e+00',axis=1)\n",
    "y_train = train[\"-1.0000000e+00\"]\n",
    "X_test = test.drop('-1.0000000e+00',axis=1)\n",
    "y_test = test[\"-1.0000000e+00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f1d0745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY:  0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators = 100)  \n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "from sklearn import metrics \n",
    "print(\"ACCURACY: \", metrics.accuracy_score(y_test, y_pred))\n",
    "accuracies.append(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eafdeb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = kfold[2]\n",
    "train = pd.concat([kfold[0],kfold[1],kfold[3]],axis=0)\n",
    "X_train = train.drop('-1.0000000e+00',axis=1)\n",
    "y_train = train[\"-1.0000000e+00\"]\n",
    "X_test = test.drop('-1.0000000e+00',axis=1)\n",
    "y_test = test[\"-1.0000000e+00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "419233e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY:  0.875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators = 100)  \n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "from sklearn import metrics \n",
    "print(\"ACCURACY: \", metrics.accuracy_score(y_test, y_pred))\n",
    "accuracies.append(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "143fa673",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = kfold[2]\n",
    "train = pd.concat([kfold[0],kfold[1],kfold[3]],axis=0)\n",
    "X_train = train.drop('-1.0000000e+00',axis=1)\n",
    "y_train = train[\"-1.0000000e+00\"]\n",
    "X_test = test.drop('-1.0000000e+00',axis=1)\n",
    "y_test = test[\"-1.0000000e+00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56c8e8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY:  0.875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators = 100)  \n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "from sklearn import metrics \n",
    "print(\"ACCURACY: \", metrics.accuracy_score(y_test, y_pred))\n",
    "accuracies.append(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c645417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75, 0.9166666666666666, 0.875, 0.875]\n",
      "0.8541666666666666\n"
     ]
    }
   ],
   "source": [
    "print(accuracies)\n",
    "print(sum(accuracies)/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a7ba89",
   "metadata": {},
   "source": [
    "## using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7093a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27fecc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY:  0.9\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('-1.0000000e+00',axis=1)\n",
    "y = df['-1.0000000e+00']\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.3, random_state=60)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators = 100)  \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"ACCURACY: \", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc07e1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 74 TEST: 25\n",
      "ACCURACY:  0.88\n",
      "TRAIN: 74 TEST: 25\n",
      "ACCURACY:  0.88\n",
      "TRAIN: 74 TEST: 25\n",
      "ACCURACY:  0.68\n",
      "TRAIN: 75 TEST: 24\n",
      "ACCURACY:  0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold # import KFold\n",
    "kf = KFold(n_splits=4,shuffle=True) #  the split - into 4 folds \n",
    "accuracies_sklearn = []\n",
    "for train_index, test_index in kf.split(X):   #kf.split splits the dataset \n",
    "    print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test  = X.iloc[test_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    y_test = y.iloc[test_index]    \n",
    "    clf = RandomForestClassifier(n_estimators = 100)  \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(\"ACCURACY: \", acc)\n",
    "    accuracies_sklearn.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3a324bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88, 0.88, 0.68, 0.8333333333333334]\n",
      "0.8183333333333334\n"
     ]
    }
   ],
   "source": [
    "print(accuracies_sklearn)\n",
    "print(sum(accuracies_sklearn)/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f90cb4",
   "metadata": {},
   "source": [
    "## Results\n",
    "The average accuracy results of Numpy are better than sklearn but not considerably much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d9bab246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average  with numpy 0.8541666666666666\n",
      "Average  with sklearn 0.8183333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"Average  with numpy\",sum(accuracies)/4)\n",
    "print(\"Average  with sklearn\",sum(accuracies_sklearn)/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4b95dd",
   "metadata": {},
   "source": [
    "Plots of Accuracies with numpy and Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e630de4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Figure Size\n",
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "plt.subplot(1, 2, 1) # row 1, col 2 index 1\n",
    "plt.title(\"With Numpy\")\n",
    "plt.xlabel(\"Test sets\")\n",
    "plt.ylabel(\"Accuracies\")\n",
    "x = np.array([\"TEST1\", \"TEST2\", \"TEST3\", \"TEST4\"])\n",
    "plt.bar(x,accuracies)\n",
    "\n",
    "plt.subplot(1, 2, 2) # row 1, col 2 index 2\n",
    "plt.title(\"With Sklearn\")\n",
    "plt.xlabel(\"Test sets\")\n",
    "plt.ylabel(\"Accuracies\")\n",
    "x = np.array([\"TEST1\", \"TEST2\", \"TEST3\", \"TEST4\"])\n",
    "plt.bar(x,accuracies_sklearn)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d644b76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
