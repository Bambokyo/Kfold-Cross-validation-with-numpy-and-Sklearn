{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "826a0897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5741aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bilal\\AppData\\Local\\Temp\\ipykernel_12644\\2671051389.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(\"ECG200_TRAIN.csv\", delimiter=\"  \")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-1.0000000e+00</th>\n",
       "      <th>5.0205548e-01</th>\n",
       "      <th>5.4216265e-01</th>\n",
       "      <th>7.2238348e-01</th>\n",
       "      <th>1.4288852e+00</th>\n",
       "      <th>2.1365158e+00</th>\n",
       "      <th>2.2811490e+00</th>\n",
       "      <th>1.9362737e+00</th>\n",
       "      <th>1.4688900e+00</th>\n",
       "      <th>1.0088451e+00</th>\n",
       "      <th>...</th>\n",
       "      <th>9.3104294e-01</th>\n",
       "      <th>6.1029836e-01</th>\n",
       "      <th>6.3889427e-01</th>\n",
       "      <th>6.8467857e-01</th>\n",
       "      <th>5.8323764e-01</th>\n",
       "      <th>6.4052167e-01</th>\n",
       "      <th>7.0858515e-01</th>\n",
       "      <th>7.0501088e-01</th>\n",
       "      <th>7.1381545e-01</th>\n",
       "      <th>4.3376464e-01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.147647</td>\n",
       "      <td>0.804668</td>\n",
       "      <td>0.367771</td>\n",
       "      <td>0.243894</td>\n",
       "      <td>0.026614</td>\n",
       "      <td>-0.274402</td>\n",
       "      <td>0.096731</td>\n",
       "      <td>-0.747731</td>\n",
       "      <td>-1.609777</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.533503</td>\n",
       "      <td>-0.400228</td>\n",
       "      <td>0.176084</td>\n",
       "      <td>1.111768</td>\n",
       "      <td>2.438428</td>\n",
       "      <td>2.734889</td>\n",
       "      <td>1.736054</td>\n",
       "      <td>0.036857</td>\n",
       "      <td>-1.265074</td>\n",
       "      <td>-0.208024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.316646</td>\n",
       "      <td>0.243199</td>\n",
       "      <td>0.370471</td>\n",
       "      <td>1.063738</td>\n",
       "      <td>1.678187</td>\n",
       "      <td>1.759558</td>\n",
       "      <td>1.697717</td>\n",
       "      <td>1.612159</td>\n",
       "      <td>1.168188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.764229</td>\n",
       "      <td>0.610621</td>\n",
       "      <td>0.552900</td>\n",
       "      <td>0.566786</td>\n",
       "      <td>0.604002</td>\n",
       "      <td>0.777068</td>\n",
       "      <td>0.812345</td>\n",
       "      <td>0.748848</td>\n",
       "      <td>0.818042</td>\n",
       "      <td>0.539347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.168874</td>\n",
       "      <td>2.075901</td>\n",
       "      <td>1.760141</td>\n",
       "      <td>1.606446</td>\n",
       "      <td>1.949046</td>\n",
       "      <td>1.302842</td>\n",
       "      <td>0.459332</td>\n",
       "      <td>0.516412</td>\n",
       "      <td>0.852180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419006</td>\n",
       "      <td>0.723888</td>\n",
       "      <td>1.323947</td>\n",
       "      <td>2.136488</td>\n",
       "      <td>1.746597</td>\n",
       "      <td>1.470220</td>\n",
       "      <td>1.893512</td>\n",
       "      <td>1.256949</td>\n",
       "      <td>0.800407</td>\n",
       "      <td>0.731540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.648658</td>\n",
       "      <td>0.752026</td>\n",
       "      <td>2.636231</td>\n",
       "      <td>3.455716</td>\n",
       "      <td>2.118157</td>\n",
       "      <td>0.520620</td>\n",
       "      <td>-0.188627</td>\n",
       "      <td>0.780818</td>\n",
       "      <td>0.933775</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097869</td>\n",
       "      <td>-0.136787</td>\n",
       "      <td>-0.340237</td>\n",
       "      <td>-0.089441</td>\n",
       "      <td>-0.080297</td>\n",
       "      <td>-0.192584</td>\n",
       "      <td>-0.304704</td>\n",
       "      <td>-0.454556</td>\n",
       "      <td>0.314590</td>\n",
       "      <td>0.582190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.404733</td>\n",
       "      <td>1.280859</td>\n",
       "      <td>2.515148</td>\n",
       "      <td>1.299519</td>\n",
       "      <td>1.453432</td>\n",
       "      <td>0.474275</td>\n",
       "      <td>-1.396562</td>\n",
       "      <td>-0.647081</td>\n",
       "      <td>0.431945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.376469</td>\n",
       "      <td>0.277811</td>\n",
       "      <td>0.225676</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.408354</td>\n",
       "      <td>0.540015</td>\n",
       "      <td>-0.027791</td>\n",
       "      <td>0.203476</td>\n",
       "      <td>0.346964</td>\n",
       "      <td>0.339185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.581277</td>\n",
       "      <td>0.876188</td>\n",
       "      <td>1.042767</td>\n",
       "      <td>1.796120</td>\n",
       "      <td>2.541399</td>\n",
       "      <td>2.246653</td>\n",
       "      <td>1.500387</td>\n",
       "      <td>1.031521</td>\n",
       "      <td>0.382672</td>\n",
       "      <td>...</td>\n",
       "      <td>1.002770</td>\n",
       "      <td>0.907869</td>\n",
       "      <td>0.916457</td>\n",
       "      <td>0.923975</td>\n",
       "      <td>0.767357</td>\n",
       "      <td>0.656223</td>\n",
       "      <td>0.762357</td>\n",
       "      <td>0.501373</td>\n",
       "      <td>-0.333336</td>\n",
       "      <td>-0.524546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.689017</td>\n",
       "      <td>2.708703</td>\n",
       "      <td>2.008381</td>\n",
       "      <td>2.235800</td>\n",
       "      <td>1.516982</td>\n",
       "      <td>0.029916</td>\n",
       "      <td>-0.561346</td>\n",
       "      <td>-0.793702</td>\n",
       "      <td>-0.979371</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.136610</td>\n",
       "      <td>-0.072176</td>\n",
       "      <td>-0.082738</td>\n",
       "      <td>-0.138468</td>\n",
       "      <td>-0.120396</td>\n",
       "      <td>-0.089411</td>\n",
       "      <td>-0.243141</td>\n",
       "      <td>-0.119710</td>\n",
       "      <td>0.124042</td>\n",
       "      <td>0.273463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.197677</td>\n",
       "      <td>0.455417</td>\n",
       "      <td>0.973110</td>\n",
       "      <td>1.935956</td>\n",
       "      <td>2.259463</td>\n",
       "      <td>1.741341</td>\n",
       "      <td>1.158296</td>\n",
       "      <td>0.418241</td>\n",
       "      <td>-0.071605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.482452</td>\n",
       "      <td>0.325569</td>\n",
       "      <td>0.247991</td>\n",
       "      <td>0.184127</td>\n",
       "      <td>0.050358</td>\n",
       "      <td>0.241988</td>\n",
       "      <td>0.331451</td>\n",
       "      <td>-0.120006</td>\n",
       "      <td>0.042423</td>\n",
       "      <td>0.343293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.179500</td>\n",
       "      <td>1.038409</td>\n",
       "      <td>1.946421</td>\n",
       "      <td>2.705141</td>\n",
       "      <td>1.670706</td>\n",
       "      <td>-0.101167</td>\n",
       "      <td>-1.578876</td>\n",
       "      <td>-0.750906</td>\n",
       "      <td>0.175310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324323</td>\n",
       "      <td>0.330489</td>\n",
       "      <td>0.111953</td>\n",
       "      <td>0.448948</td>\n",
       "      <td>0.567132</td>\n",
       "      <td>0.136757</td>\n",
       "      <td>0.444768</td>\n",
       "      <td>0.151050</td>\n",
       "      <td>0.193378</td>\n",
       "      <td>0.451709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.073124</td>\n",
       "      <td>0.776054</td>\n",
       "      <td>2.181336</td>\n",
       "      <td>3.440325</td>\n",
       "      <td>2.168475</td>\n",
       "      <td>0.497315</td>\n",
       "      <td>-0.924284</td>\n",
       "      <td>-1.499227</td>\n",
       "      <td>-0.679328</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058935</td>\n",
       "      <td>-0.130638</td>\n",
       "      <td>-0.347235</td>\n",
       "      <td>-0.177933</td>\n",
       "      <td>-0.060332</td>\n",
       "      <td>-0.347634</td>\n",
       "      <td>-0.447443</td>\n",
       "      <td>-0.066689</td>\n",
       "      <td>-0.178448</td>\n",
       "      <td>-0.256052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    -1.0000000e+00   5.0205548e-01   5.4216265e-01   7.2238348e-01  \\\n",
       "0              1.0        0.147647        0.804668        0.367771   \n",
       "1             -1.0        0.316646        0.243199        0.370471   \n",
       "2             -1.0        1.168874        2.075901        1.760141   \n",
       "3              1.0        0.648658        0.752026        2.636231   \n",
       "4              1.0        0.404733        1.280859        2.515148   \n",
       "..             ...             ...             ...             ...   \n",
       "94             1.0        0.581277        0.876188        1.042767   \n",
       "95            -1.0        2.689017        2.708703        2.008381   \n",
       "96            -1.0        0.197677        0.455417        0.973110   \n",
       "97             1.0        0.179500        1.038409        1.946421   \n",
       "98             1.0        0.073124        0.776054        2.181336   \n",
       "\n",
       "     1.4288852e+00   2.1365158e+00   2.2811490e+00   1.9362737e+00  \\\n",
       "0         0.243894        0.026614       -0.274402        0.096731   \n",
       "1         1.063738        1.678187        1.759558        1.697717   \n",
       "2         1.606446        1.949046        1.302842        0.459332   \n",
       "3         3.455716        2.118157        0.520620       -0.188627   \n",
       "4         1.299519        1.453432        0.474275       -1.396562   \n",
       "..             ...             ...             ...             ...   \n",
       "94        1.796120        2.541399        2.246653        1.500387   \n",
       "95        2.235800        1.516982        0.029916       -0.561346   \n",
       "96        1.935956        2.259463        1.741341        1.158296   \n",
       "97        2.705141        1.670706       -0.101167       -1.578876   \n",
       "98        3.440325        2.168475        0.497315       -0.924284   \n",
       "\n",
       "     1.4688900e+00   1.0088451e+00  ...   9.3104294e-01   6.1029836e-01  \\\n",
       "0        -0.747731       -1.609777  ...       -0.533503       -0.400228   \n",
       "1         1.612159        1.168188  ...        0.764229        0.610621   \n",
       "2         0.516412        0.852180  ...        0.419006        0.723888   \n",
       "3         0.780818        0.933775  ...       -0.097869       -0.136787   \n",
       "4        -0.647081        0.431945  ...        0.376469        0.277811   \n",
       "..             ...             ...  ...             ...             ...   \n",
       "94        1.031521        0.382672  ...        1.002770        0.907869   \n",
       "95       -0.793702       -0.979371  ...       -0.136610       -0.072176   \n",
       "96        0.418241       -0.071605  ...        0.482452        0.325569   \n",
       "97       -0.750906        0.175310  ...        0.324323        0.330489   \n",
       "98       -1.499227       -0.679328  ...       -0.058935       -0.130638   \n",
       "\n",
       "     6.3889427e-01   6.8467857e-01   5.8323764e-01   6.4052167e-01  \\\n",
       "0         0.176084        1.111768        2.438428        2.734889   \n",
       "1         0.552900        0.566786        0.604002        0.777068   \n",
       "2         1.323947        2.136488        1.746597        1.470220   \n",
       "3        -0.340237       -0.089441       -0.080297       -0.192584   \n",
       "4         0.225676        0.159091        0.408354        0.540015   \n",
       "..             ...             ...             ...             ...   \n",
       "94        0.916457        0.923975        0.767357        0.656223   \n",
       "95       -0.082738       -0.138468       -0.120396       -0.089411   \n",
       "96        0.247991        0.184127        0.050358        0.241988   \n",
       "97        0.111953        0.448948        0.567132        0.136757   \n",
       "98       -0.347235       -0.177933       -0.060332       -0.347634   \n",
       "\n",
       "     7.0858515e-01   7.0501088e-01   7.1381545e-01   4.3376464e-01  \n",
       "0         1.736054        0.036857       -1.265074       -0.208024  \n",
       "1         0.812345        0.748848        0.818042        0.539347  \n",
       "2         1.893512        1.256949        0.800407        0.731540  \n",
       "3        -0.304704       -0.454556        0.314590        0.582190  \n",
       "4        -0.027791        0.203476        0.346964        0.339185  \n",
       "..             ...             ...             ...             ...  \n",
       "94        0.762357        0.501373       -0.333336       -0.524546  \n",
       "95       -0.243141       -0.119710        0.124042        0.273463  \n",
       "96        0.331451       -0.120006        0.042423        0.343293  \n",
       "97        0.444768        0.151050        0.193378        0.451709  \n",
       "98       -0.447443       -0.066689       -0.178448       -0.256052  \n",
       "\n",
       "[99 rows x 97 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"ECG200_TRAIN.csv\", delimiter=\"  \")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4581a021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0000000e+00    0\n",
       " 5.0205548e-01    0\n",
       " 5.4216265e-01    0\n",
       " 7.2238348e-01    0\n",
       " 1.4288852e+00    0\n",
       "                 ..\n",
       " 6.4052167e-01    0\n",
       " 7.0858515e-01    0\n",
       " 7.0501088e-01    0\n",
       " 7.1381545e-01    0\n",
       " 4.3376464e-01    0\n",
       "Length: 97, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d4b94e",
   "metadata": {},
   "source": [
    "## Sparating the dataset with labels -1s and 1s, 70% for ones and 30% for the minus ones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b93084b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = df.sort_values(by='-1.0000000e+00',ignore_index=True)\n",
    "af = np.where(s['-1.0000000e+00'] == -1)\n",
    "minusones = s[:30]\n",
    "ones = s[30:]\n",
    "kfold = []\n",
    "len(ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a843bef0",
   "metadata": {},
   "source": [
    "## Making 4 patches of 1s and -1s\n",
    "patches1 --> 1s  \n",
    "\n",
    "patches2 --> -1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5cf2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 0\n",
    "patches1 = []\n",
    "for i in range(17,len(ones)+1,17):\n",
    "    k = ones[g:i]\n",
    "    patches1.append(k)\n",
    "    g = i\n",
    "    \n",
    "g = 0\n",
    "patches2 = []\n",
    "for i in range(7,len(minusones)+1,7):\n",
    "    k = minusones[g:i]\n",
    "    patches2.append(k)\n",
    "    g = i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d53af9e",
   "metadata": {},
   "source": [
    "Concatenating the datesets (patches) of 1s and -1s and shuffling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7ed5b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,4):\n",
    "    biji = pd.concat([patches1[i],patches2[i]],axis=0)\n",
    "    kfold.append(biji)\n",
    "\n",
    "# shuffleing\n",
    "for i in range(0,4):\n",
    "    kfold[i] = kfold[i].sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc1da8e",
   "metadata": {},
   "source": [
    "## Calculating the accuracies of differnt K-folds on machine learning Model (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "430e9b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7bc7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = kfold[0]\n",
    "train = pd.concat([kfold[1],kfold[2],kfold[3]],axis=0)\n",
    "X_train = train.drop('-1.0000000e+00',axis=1)\n",
    "y_train = train[\"-1.0000000e+00\"]\n",
    "\n",
    "X_test = test.drop('-1.0000000e+00',axis=1)\n",
    "y_test = test[\"-1.0000000e+00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f7943a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY:  0.7916666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators = 100)  \n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "from sklearn import metrics \n",
    "print(\"ACCURACY: \", metrics.accuracy_score(y_test, y_pred))\n",
    "accuracies.append(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c396dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = kfold[1]\n",
    "train = pd.concat([kfold[0],kfold[2],kfold[3]],axis=0)\n",
    "X_train = train.drop('-1.0000000e+00',axis=1)\n",
    "y_train = train[\"-1.0000000e+00\"]\n",
    "X_test = test.drop('-1.0000000e+00',axis=1)\n",
    "y_test = test[\"-1.0000000e+00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f1d0745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY:  0.875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators = 100)  \n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "from sklearn import metrics \n",
    "print(\"ACCURACY: \", metrics.accuracy_score(y_test, y_pred))\n",
    "accuracies.append(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eafdeb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = kfold[2]\n",
    "train = pd.concat([kfold[0],kfold[1],kfold[3]],axis=0)\n",
    "X_train = train.drop('-1.0000000e+00',axis=1)\n",
    "y_train = train[\"-1.0000000e+00\"]\n",
    "X_test = test.drop('-1.0000000e+00',axis=1)\n",
    "y_test = test[\"-1.0000000e+00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "419233e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY:  0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators = 100)  \n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "from sklearn import metrics \n",
    "print(\"ACCURACY: \", metrics.accuracy_score(y_test, y_pred))\n",
    "accuracies.append(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "143fa673",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = kfold[2]\n",
    "train = pd.concat([kfold[0],kfold[1],kfold[3]],axis=0)\n",
    "X_train = train.drop('-1.0000000e+00',axis=1)\n",
    "y_train = train[\"-1.0000000e+00\"]\n",
    "X_test = test.drop('-1.0000000e+00',axis=1)\n",
    "y_test = test[\"-1.0000000e+00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56c8e8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY:  0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators = 100)  \n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "from sklearn import metrics \n",
    "print(\"ACCURACY: \", metrics.accuracy_score(y_test, y_pred))\n",
    "accuracies.append(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c645417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7916666666666666, 0.875, 0.8333333333333334, 0.8333333333333334]\n",
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "print(accuracies)\n",
    "print(sum(accuracies)/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a7ba89",
   "metadata": {},
   "source": [
    "## using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7093a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27fecc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY:  0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('-1.0000000e+00',axis=1)\n",
    "y = df['-1.0000000e+00']\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.3, random_state=60)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators = 100)  \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"ACCURACY: \", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01d59dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(x):\n",
    "    c1 = 0\n",
    "    c2 = 0\n",
    "    for i in x:\n",
    "        if i == -1:\n",
    "            c1+=1\n",
    "        else:\n",
    "            c2+=1\n",
    "    print('1s', c1)\n",
    "    print('-1s', c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc07e1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 74 TEST: 25\n",
      "1s 21\n",
      "-1s 53\n",
      "ACCURACY:  0.72\n",
      "TRAIN: 74 TEST: 25\n",
      "1s 22\n",
      "-1s 52\n",
      "ACCURACY:  0.84\n",
      "TRAIN: 74 TEST: 25\n",
      "1s 24\n",
      "-1s 50\n",
      "ACCURACY:  0.84\n",
      "TRAIN: 75 TEST: 24\n",
      "1s 23\n",
      "-1s 52\n",
      "ACCURACY:  0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold # import KFold\n",
    "kf = KFold(n_splits=4,shuffle=True) #  the split - into 4 folds \n",
    "accuracies_sklearn = []\n",
    "for train_index, test_index in kf.split(X):   #kf.split splits the dataset \n",
    "    print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test  = X.iloc[test_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    y_test = y.iloc[test_index]\n",
    "    check(y_train)\n",
    "    clf = RandomForestClassifier(n_estimators = 100)  \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(\"ACCURACY: \", acc)\n",
    "    accuracies_sklearn.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a324bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8, 0.8, 0.72, 0.875]\n",
      "0.7987500000000001\n"
     ]
    }
   ],
   "source": [
    "print(accuracies_sklearn)\n",
    "print(sum(accuracies_sklearn)/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f90cb4",
   "metadata": {},
   "source": [
    "## Results\n",
    "The average accuracy results of Numpy are better than sklearn but not considerably much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9bab246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average  with numpy 0.8333333333333334\n",
      "Average  with sklearn 0.7987500000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"Average  with numpy\",sum(accuracies)/4)\n",
    "print(\"Average  with sklearn\",sum(accuracies_sklearn)/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4b95dd",
   "metadata": {},
   "source": [
    "Plots of Accuracies with numpy and Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e630de4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Figure Size\n",
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "plt.subplot(1, 2, 1) # row 1, col 2 index 1\n",
    "plt.title(\"With Numpy\")\n",
    "plt.xlabel(\"Test sets\")\n",
    "plt.ylabel(\"Accuracies\")\n",
    "x = np.array([\"TEST1\", \"TEST2\", \"TEST3\", \"TEST4\"])\n",
    "plt.bar(x,accuracies)\n",
    "\n",
    "plt.subplot(1, 2, 2) # row 1, col 2 index 2\n",
    "plt.title(\"With Sklearn\")\n",
    "plt.xlabel(\"Test sets\")\n",
    "plt.ylabel(\"Accuracies\")\n",
    "x = np.array([\"TEST1\", \"TEST2\", \"TEST3\", \"TEST4\"])\n",
    "plt.bar(x,accuracies_sklearn)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d644b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f899adc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
